{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbead18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import to_absolute_path as abspath\n",
    "from omegaconf import DictConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "\n",
    "from src.data.text_processing import TextProcessing\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from datasets.filesystems.s3filesystem import S3FileSystem\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4abd9",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Loading available datasets and  transforming it to pandas dataframe with columns\n",
    "`text` and `label`, where the label shows if the text is on-topic (1) or\n",
    "off-topic (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf8ccc",
   "metadata": {},
   "source": [
    "Data obtained from the supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg: DictConfig = compose(config_name=\"config\")\n",
    "    supervisor_tweets_path = abspath(\"../\" + cfg.supervisor.tweets)\n",
    "    alberta_path = abspath(\"../\" + cfg.alberta.raw)\n",
    "    queensland_path = abspath(\"../\" + cfg.queensland.raw)\n",
    "\n",
    "with open(supervisor_tweets_path, \"r\") as file:\n",
    "    tweets_json = json.load(file)\n",
    "supervisor_df = pd.json_normalize(list(tweets_json.values()))\n",
    "print(supervisor_df.shape)\n",
    "# NOTE: Some tweets, don't have geo-tagging or attachements, hence the NaN values\n",
    "supervisor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_df = supervisor_df[[\"id\", \"text_en\", \"On Topic\"]]\n",
    "supervisor_df = supervisor_df.rename(columns={\"text_en\": \"text\", \"On Topic\": \"label\"})\n",
    "supervisor_df = supervisor_df[supervisor_df[\"label\"] != \"\"]\n",
    "supervisor_df = supervisor_df.astype({\"label\": \"int\", \"id\": \"int\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b6fde",
   "metadata": {},
   "source": [
    "Data obtaine from [CrisisLex: Download Crisis-Related Collections](https://crisislex.org/data-collections.html#CrisisLexT6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_df = pd.read_csv(queensland_path)\n",
    "alberta_df = pd.read_csv(alberta_path)\n",
    "\n",
    "print(f\"queensland {queensland_df.shape}\")\n",
    "print(f\"alberta {alberta_df.shape}\")\n",
    "# NOTE: Some tweets, don't have geo-tagging or attachements, hence the NaN values\n",
    "queensland_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_crisislex_data(df):\n",
    "    df = df.rename(columns={\"tweet_id\": \"id\", \"tweet\": \"text\"})\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == \"on-topic\" else 0)\n",
    "    df[\"id\"] = df[\"id\"].apply(lambda x: x[1:-1])\n",
    "    df = df.astype({\"id\": \"int\"})\n",
    "    return df\n",
    "\n",
    "\n",
    "queensland_df = process_crisislex_data(queensland_df)\n",
    "alberta_df = process_crisislex_data(alberta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed = pd.concat([queensland_df, alberta_df, supervisor_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958abd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove retweets\n",
    "df_needed = df_needed[df_needed[\"text\"].str[:2] != \"RT\"]\n",
    "# Remove duplicates\n",
    "df_needed = df_needed.drop_duplicates()\n",
    "\n",
    "text_preprocessing = TextProcessing()\n",
    "# Clean text\n",
    "df_needed[\"text\"] = text_preprocessing.clean_text(df_needed[\"text\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_needed)\n",
    "\n",
    "dataset = dataset.remove_columns([\"id\"])\n",
    "\n",
    "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "train_test_valid_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_testvalid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"valid\": test_valid[\"train\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681115f",
   "metadata": {},
   "source": [
    "## Finetuning the model\n",
    "\n",
    "Use hugging face library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "train_test_valid_dataset_tokenized = train_test_valid_dataset.map(\n",
    "    tokenize, batched=True, batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229a926",
   "metadata": {},
   "source": [
    "\n",
    "TODO: Try too use sagemaker to fine tune the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sage_maker')['Role']['Arn']\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test_valid_dataset_tokenized['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0626bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem()\n",
    "s3_prefix = 'samples/datasets/floods'\n",
    "\n",
    "train_dataset = train_test_valid_dataset_tokenized['train']\n",
    "test_dataset = train_test_valid_dataset_tokenized['test']\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n",
    "test_dataset = test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\n",
    "        'torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\n",
    "        'torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94522b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "        'epochs': 1,\n",
    "        'train_batch_size': 32,\n",
    "        'model_name': model_ckpt}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.12',\n",
    "        pytorch_version='1.9',\n",
    "        py_version='py38',\n",
    "        hyperparameters=hyperparameters)\n",
    "\n",
    "huggingface_estimator.fit(\n",
    "        {'train': training_input_path, 'test': test_input_path})"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
