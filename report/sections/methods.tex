% vim: spell spelllang=en_gb
\chapter{Methods}

This section discusses and motivates the methods used in the project. Figure~\ref{fig:flow_chart}
shows a flow chart of the steps for the pipeline (an enlarged and more detailed copy is available in
Figure~\ref{fig:flow_chart_big} of Appendix \ref{appendix:raw}). The pipeline consists of the
following steps: Data collection, text classification, location extraction, text analysis, and visualization.
Python is the primary programming language used for the project because of the rich ecosystem
surrounding it, especially when it comes to data science-related tasks. The code base is available
on a GitHub
repository\footnote{https://github.com/YasserKa/Classification-and-visualization-of-natural-disasters-using-Twitter}
accompanied with a \texttt{README.md} containing instructions to set up the environment and run the
project.

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\columnwidth]{./images/pipeline_concise.png}
\end{center}
\caption{Flow chart for the pipeline}
\label{fig:flow_chart}
\end{figure}

\section{Data Collection}

Finding a good quality data source is the first step to having a lean start for most research
questions. The pipeline trains an \ac{ML} classifier using three manually labelled datasets. Two of
them are crowdsourced datasets provided by CrisisLex6; the tweets are from the 2013 flood events in
Alberta\footnote{https://en.wikipedia.org/wiki/2013\_Alberta\_floods} and
Queensland\footnote{https://en.wikipedia.org/wiki/Cyclone\_Oswald}, and there are around 10,000
records for each one with the tweet's ID, tweet's text, and a label about the relevance of the tweet
regarding the event. The third dataset was collected and annotated in the scope of the
AI4ClimateAdaptation project and includes flood events in Sweden, spanning between 2015 and 2021; it
contains 4899 tweets, most of them are in the Swedish language, with attributes presented in
Table~\ref{tab:dataset_attr}. The text and metadata of the tweets are extracted from Twitter's
\ac{API} using the IDs. The trained model performance is verified using extracted tweets from the
\ac{API} using Tweepy\footnote{https://docs.tweepy.org/en/latest/index.html}, a python library for
accessing Twitter \ac{API}. Figure~\ref{fig:flow_chart_data_collection_text_classification} shows
both the source and usage of the data in the pipeline.

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\columnwidth]{./images/data_collection_text_classification.png}
\end{center}
\caption{Data collection and text classification steps of the pipeline}
\label{fig:flow_chart_data_collection_text_classification}
\end{figure}

\begin{table}
  \center
  \caption{Dataset attributes}
  \begin{tabular}{|l|l|l|}
    \hline
    Field & Type & Description \\
    \hline
    ID & Int & ID of the tweet\\
    \hline
    On Topic  & Bool & Text discusses an event \\
    \hline
    Informative sarcastic  & Bool & Text contains relevant information about the event \\
    \hline
    Contains IMPACT info & Bool & Text discusses the impact of the event \\
    \hline
    Explicit location & Bool & Text mentions the location of the event \\
    \hline
  \end{tabular}
  \label{tab:dataset_attr}
\end{table}

After retrieving the data, they are pre-processed to prepare them for the upcoming tasks, such as
training an \ac{ML} algorithm, text analysis, and visualization. Parts of the text that do not
contribute to the context are discarded: \ac{URL}s, emojis, mentions, hashtag signs, numbers, new
lines, punctuation, and stopwords (provided by spaCy\footnote{https://spacy.io/}, an \ac{NLP} python
library). Afterwards, duplicate tweets, tweets with no text, and retweets are discarded from the
dataset. The trained model requires the text to be in English, and since Sweden is the focus of the
research, most of the text is in Swedish; thus, the Swedish text is translated to English using
google translate\footnote{https://translate.google.com/} by a python library wrapper
deep-translate\footnote{https://deep-translator.readthedocs.io/en/latest/}. 

Data needs to be stored and managed to accommodate policies and regulations. Twitter's developer
policy\footnote{https://developer.twitter.com/en/developer-terms/policy} has a content
redistribution section stating that only tweets' IDs can be shared online. Thus, the tweets can not
be available publicly on the internet, such as GitHub (the service that hosts the publicly available
code base). To this end, the data is stored after each step of the pipeline using google drive using
\ac{DVC}'s\footnote{https://dvc.org/doc} data management capabilities.

Twitter's \ac{API} provides an extensive list of information about the
tweets\footnote{https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet}.
It shares the engagement metrics of the tweet, including like count, reply count, and retweet count;
as well as, an \ac{NLP} analysis of its own, such as the language used, and entities parsed from the
text. Table~\ref{tab:tweet_attr} shows the tweet's attributes used in this project for the following
reasons: the ID to generate the \ac{URL} of the tweet, the text for \ac{NLP} tasks, the created date
for temporal analysis, and author's id to reduce spam.

\begin{table}
  \center
  \caption{Tweet attributes used}
  \begin{tabular}{|l|l|l|}
    \hline
    Attribute & Type & Description \\
    \hline
    id & Int & The unique identifier of the requested Tweet \\
    \hline
    text & Str & The actual UTF-8 text of the Tweet \\
    \hline
    created at & Date  & Creation time of the Tweet \\
    \hline
    author id & Str & The unique identifier of the tweet creator \\
    \hline
  \end{tabular}
  \label{tab:tweet_attr}
\end{table}

\section{Text Classification}\label{sec:text_classification_section}

This project uses the DistilBERT transformer~\cite{Sanh2019DistilBERTAD}, a variant of \ac{BERT}, for
text classification. The main advantage of this model is that it achieves comparable performance to
BERT while being significantly smaller than BERT and more efficient. The used DistilBERT pre-trained
model is provided by Hugging Face\footnote{https://huggingface.co/}, a framework that provides a
unified \ac{API} for over more than 50 architectures, making it easier for users to integrate
\ac{NLP} models into their applications. The learning rate for the neural network is $5\times
e^{-5}$ with 100 warmup steps over four epochs using 90\% of the labelled tweets as training data,
5\% as test data, and 5\% for validation. Text classification's purpose is to identify the tweets
that discuss flood events, so model training is done with the ``On Topic'' attribute as a label. 

Training the model in the local environment takes a long time with the available resources, so the
training is done using Amazon SageMaker\footnote{https://aws.amazon.com/sagemaker/}, a service that
covers tools to build, train, and deploy \ac{ML} models, to accelerate the process. The data is
uploaded to Amazon Simple Storage Service (Amazon S3) to make it accessible for the Hugging Face
training script, which gets executed in an instance available in the cloud. After completing the
training, the fine-tuned model and the evaluation metrics are downloaded. The evaluation
metrics~\cite{sokolovaSystematicAnalysisPerformance2009} consist of the following:

\begin{itemize}
  \item Confusion matrix: a matrix showing the classifier's predictions for a labelled dataset
    corresponding to its actual values (Table~\ref{tab:confusion_matrix}).
    

    \begin{table}
      \center
      \caption{Confusion matrix}
      \bgroup
      \def\arraystretch{1.5}
      \begin{tabular}{@{}cc|cc@{}}
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{Predicted} \\ 
        \multicolumn{1}{c}{} & 
        \multicolumn{1}{c|}{} & 
        \multicolumn{1}{c}{Positive} & 
        \multicolumn{1}{c}{Negative} \\ 
        \cline{2-4}
        \multirow[c]{2}{*}{\rotatebox[origin=tr]{90}{Actual}}
                                     & Positive  & \ac{TP} & \ac{FN}   \\
                                     \cline{2-4}
                                     & Negative  & \ac{FP} & \ac{TN} \\
                                     \cline{2-4}
      \end{tabular}
      \egroup
      \label{tab:confusion_matrix}
    \end{table}

  \item Accuracy: a fraction of the number of correctly classified instances (i.e., true positives
    and true negatives) among all instances (i.e., whole dataset) (equation~\ref{eq:accuracy}).
    \begin{equation}
      \text{Accuracy}=\frac{TN+TP}{TN+FN+TP+FP} 
      \label{eq:accuracy}
    \end{equation}
  \item Precision: a fraction of the number of correctly classified relevant instances (i.e., true
    positives) among the total number of instances classified as relevant (i.e., true positives and
    false positives) (equation~\ref{eq:precision}).
    \begin{equation}
      \text{Precision}=\frac{TP}{TP+FP} 
      \label{eq:precision}
    \end{equation}
  \item Recall: a fraction of the correctly classified relevant instances (i.e., true positives)
    among all relevant instances (i.e. true positives and false negatives) (equation~\ref{eq:recall}).
    \begin{equation}
      \text{Recall}=\frac{TP}{TP+FN} 
      \label{eq:recall}
    \end{equation}
  \item $\text{F}_1$ score: a harmonic mean of precision and recall (equation~\ref{eq:f1}).
    \begin{equation}
      \text{F}_1 =2\cdot\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}} 
      \label{eq:f1}
    \end{equation}
\end{itemize}


\section{Location Extraction}

The project uses a hybrid approach for geoparsing to extract locations. For toponym recognition, the
tokens representing locations are identified using the KBLab/bert-base-swedish-cased-ner
model\footnote{https://huggingface.co/KBLab/bert-base-swedish-cased-ner}, which is based on BERT and
fine-tuned for \ac{NER} using The Stockholm-Ume√• Corpus, a collection of Swedish texts from the
1990s that consists of one million words. As for toponym resolution, the location tokens are
disambiguated using Nominatim and GeoNames geocoders through
Geopy\footnote{https://geopy.readthedocs.io/en/latest}, a Python client for several popular
geocoding web services. Nominatim retrieves different fields about the location from OpenStreetMap.
An output example is available in Appendix \ref{appendix:examples}. The descriptions for the
attributes are available in the
documentation\footnote{https://nominatim.org/release-docs/develop/api/Output/}. The project uses the
\texttt{lat}, \texttt{lon}, and \texttt{display\_name} to represent the location on a map. In some cases, the text might
contain two locations, the one with the smaller bounding box (area of corner coordinates) is used,
which is, in most cases, a more specific place located in the bigger one (e.g. a street within a
municipality). The geocoder services provide the ability to limit the search of locations within a
specific country. Since the project is limited to Sweden, the output is limited using this option,
reducing the false positives that happen when different countries have places with the same name.
Tweets not containing location terms identifying a geographical location are discarded, as shown in
Figure~\ref{fig:flow_chart_location_extraction}.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\columnwidth]{./images/location_extraction.png}
  \end{center}
  \caption{Flow chart for the location extraction step of the pipeline}
  \label{fig:flow_chart_location_extraction}
\end{figure}


\section{Text analysis}

Further pre-processing is done on the dataset to prepare for text analysis tasks. Lemmatisation is
done on the text, using \ac{NLTK}\footnote{https://www.nltk.org/}, to reduce words to their lemmas;
then, the corpus is tokenized. Terms occurring in less than 20 documents or 5\% of
the documents are removed, as well as the terms mentioned in more than 75\% of the documents.
Bigrams that occur more than 20 times in the corpus are included, such as traffic jam, and climate
change. To reduce the impact of spambots, tweets created by the same user who tweeted about the same location the past
week are discarded.

The text analysis techniques used in this project are \ac{LDA}
\cite{falushInferencePopulationStructure2003}\cite{pritchardInferencePopulationStructure2000},
\ac{TF-IDF}~\cite{saltonTermweightingApproachesAutomatic1988}, and
\ac{t-SNE}~\cite{vandermaatenVisualizingHighDimensionalData2008}. \ac{LDA} is a topic modelling
method that generates topics (a set of terms) in a corpus and assigns the relevance of each topic in
each document. Categorizing tweets can show important insights about a flood event, such as
incidents caused by floods and their impact on society. The \ac{LDA} model is initialized and
trained using Gensim \cite{rehurek_lrec}, where the number of discovered topics is adjustable in the
visualization. The second text analysis technique used is \ac{TF-IDF}, using
scikit-learn\footnote{https://scikit-learn.org/stable/}, to extract interesting terms by checking
their average weight and frequency in the corpus. Lastly, \ac{t-SNE} is a visualization method for
high-dimensional data by reducing their dimensions to two or three-dimensional maps. In this
project, \ac{t-SNE} reduces the dimensions of a \ac{TF-IDF} matrix generated from the corpus to
2-dimensional space while using the euclidean distance between tweets; afterwards, the tweets are
presented on a scatter plot with clusters generated by \ac{DBSCAN}
\cite{esterDensitybasedAlgorithmDiscovering1996}, a density-based clustering non-parametric
algorithm. Applying dimensionality reduction on data reduces their information, so the clustering is
done before applying \ac{t-SNE} on the tweets. Presenting the tweets using a clustered 2-dimensional
space makes it easier to explore tweets for insights about the discussed flood event, such as
accidents and infrastructure damages.

\section{Visualization} 

Visualization is the final and most significant step of the pipeline since it provides a framework
for domain experts to interpret the data and gain actionable insights. Given the nature of the
problem the pipeline addresses, which is extracting information about disastrous events, it is
crucial to have several plots representing the different aspects of the events. Most hazards impact
certain regions during a time interval, so describing the spatio-temporal information of the events
is needed to analyse them. Also, the tweets being mainly text allows them to be portrayed visually
after applying text analysis techniques. Another direct and beneficial use for visualizing the data
is to validate that the pipeline is functioning as intended by navigating through the plots and
checking for suspicious results. The web application proposed in this thesis is made by
Dash\footnote{https://dash.plotly.com/} and Plotly\footnote{https://plotly.com/python/} python
packages. Dash Bootstrap
Components\footnote{https://dash-bootstrap-components.opensource.faculty.ai/} is used as well for an
easier way to use Bootstrap components for Plotly Dash, such as buttons, input, and tables.

Figure~\ref{fig:visual_interface} shows the visual interface containing all the graphs enabling
spatial, temporal, and textual exploration of the tweets: (A) a table showing tweets' properties,
(B) a map containing clusters of tweets, (C) a scatter plot for 2-dimensional representation of
tweets, (D) tables populated with terms mentioned in the tweets, and (E) a histogram for tweets'
creation dates. Users can add filtering rules for the tweets in all the plots using their creation
dates, location, and textual properties.

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\columnwidth]{./images/visual_interface.png}
\end{center}
\caption{The proposed visual interface}
\label{fig:visual_interface}
\end{figure}

The visual interface is designed to fulfil the basic principles for the Visual Information Seeking
Mantra: ``Overview first, zoom and filter, then details-on-demand'' \cite{shneidermanEyesHaveIt1996}; most of the plots
adhere to five of the seven tasks mentioned in the same paper:
\begin{itemize}
  \item Overview: Having an overview of the whole dataset.
  \item Zoom: Zooming in on an entry of interest.
  \item Filter: Disregarding uninteresting entries.
  \item Details-on-demand: Getting details on a selected entry or group when needed.
  \item Relate: Viewing the connections among entries.
\end{itemize}

The metadata in Figure~\ref{fig:meta_data}, which are displayed in the interface between the map (B)
and histogram (E) in Fig. \ref{fig:visual_interface}, handles the overview task for the entire
dataset. It contains the total number of tweets, the number of selected tweets, the oldest and
newest tweet creation dates, the total number of locations, the number of selected locations, and a
list of locations' names with the number of their occurrence with an ``etc.'' button to show a
pop-over with the rest of the locations.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=\columnwidth]{./images/meta_data.png}
    \end{center}
    \caption{Metadata about the visual interface}
    \label{fig:meta_data}
\end{figure}


Zooming helps the users to focus on the portion of tweets they are interested in, and Plotly
supports it using the cursor for the map, histogram, and scatter plots. All plots retain the
filtering done on each one of them, so the information of the selected tweets is presented after
each filtering step, making the selected tweets easier to compare and explore; thus, satisfying the
``Details-on-demand'' and ``Relate'' tasks.

Figure~\ref{fig:map} shows the spatial distribution of tweets using an interactive map containing
clickable clustered pointers for the tweets. It makes finding locations mentioned in the tweets more
intuitive than using their toponymy. Zooming in or out disperses or congregates clusters,
respectively, providing enough clusters at any given moment. Hovering over a map pointer
shows a pop-up with the location name extracted by the pipeline. Clicking on a cluster or a region
selects the tweets they contain; this will zoom in to cover them while filtering out the unselected
pointers from the map. The top right section of the map has several elements: text elements showing
the name of the hovered and selected regions, a reset button to remove the current filter, and a
radio element to change the spatial resolution of the available regions between counties and
municipalities, enabling different granularities to filter tweets.

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\columnwidth]{./images/map.png}
\end{center}
\caption{Map showing clusters of tweets}
\label{fig:map}
\end{figure}

Another way to explore the tweets is by showing the temporal distribution of their creation dates
using a histogram as seen in Figure~\ref{fig:histogram}. Event detection is a good use for
histograms since they attract the attention of individuals, leading to sudden peaks that dissipate
afterwards. The dates are aggregated by day if they span a month or less; otherwise, by month.
Tweets selection is done using a select box between two dates, where the blue and red parts of the
bars represent the selected and unselected tweets, respectively. Hovering over the bars shows the
date and the number of tweets.

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\columnwidth]{./images/histogram.png}
\end{center}
\caption{Histogram for tweets' creation dates}
\label{fig:histogram}
\end{figure}

Tables show all the properties for data in a straightforward way to find patterns while searching
and comparing the entries among each other. The table in Figure~\ref{fig:tweets_table} shows the
selected tweets with several of their attributes: the ID, the raw (original) text, the translated
text, the processed text, the hashtags used, the location extracted, the softmax value for the
prediction, and the creation date. The table is sortable and paginated, yet it lacks the features to
satisfy the ``Filtering'' task.

\begin{figure}[ht!]
\begin{center}
  \includegraphics[width=\columnwidth]{./images/tweets_table.png}
\end{center}
\caption{Table showing the tweets}
\label{fig:tweets_table}
\end{figure}

Textual data are hard to visualize, requiring a processing step to reduce their complexity;
dimensionality reduction transforms the data from a high-dimensional space to a lower one to
represent it visually using plots. Figure~\ref{fig:scatter} shows a scatter plot for the
\ac{t-SNE}'s 2-dimensional space representation of tweets using the euclidean space with \ac{DBSCAN}
clustering, as discussed in the previous section. The clusters can be recalculated after adjusting
the properties using the text inputs above the scatter plot: ``eps'', the maximum distance between
two samples for one to be considered as in the neighbourhood of the other; and ``minimum samples'',
The number of samples (or total weight) in a neighbourhood for a point to be considered as a core
point including the point itself. Hovering over the points show a pop-up of the text for the tweets,
and the points can be selected using a box or lasso selection, meeting the ``Filtering'' task.

\begin{figure}[H]
\begin{center}
  \includegraphics[width=0.5\columnwidth]{./images/scatter.png}
\end{center}
\caption{Scatter plot for t-SNE's space}
\label{fig:scatter}
\end{figure}

The results of \ac{LDA} and \ac{TF-IDF} are displayed in two tables (shown in
Figure~\ref{fig:lda_table} and Figure~\ref{fig:tfidf_table}, respectively)  showing the frequency of
the terms and their mean weights. The tables provide a suitable presentation to explore terms by
checking their frequency and the topics they belong to. Users can alter the number of topics
generated by \ac{LDA} using a text input and regenerate the tables on the selected tweets by
clicking the button. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/lda_topics.png}
        \caption{\ac{LDA} topic weights}
        \label{fig:lda_table}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 0.5cm 0 0},clip]{./images/tf_idf.png}
        \caption{\ac{TF-IDF} weights}
        \label{fig:tfidf_table}
    \end{subfigure}
    \caption{Tables showing terms with respect their frequency and their weights}
    \label{fig:tables_lda_tfidf}
\end{figure}

\section{Reliability, Validity, and Ethics}

Any quantitative search project has to state certain aspects of the methods used: reliability,
validity, and ethics \cite{woodrowReliabilityValidityEthics2014}. ``Reliability'' dictates that the
results should be similar if done in the same environment. ``Validity'' shows how credible the
research is (i.e., internal) and how it generalizes to other situations (i.e., external). ``Ethics''
makes sure that the methods don't impact society negatively. 

The methods used in the pipeline return the same results if applied to a specific dataset, making
them reliable. Data extraction depends on a query that returns a set of tweets from the \ac{API};
the users who created these tweets can edit, remove, or make them private, altering the request's
results. The pipeline mitigates this problem by retrieving and storing the tweets once in files,
keeping them out of reach from external effects. Tweet's classification depends on an artificial
neural network which relies on randomly initialized tweets that are adjusted using a stochastic
procedure during the training process to categorize the tweets. In other words,  some tweets are
categorized differently after each training attempt. Unless there is a reason to re-train the model,
such as improving data quality, the classification task will use the same trained model, resulting
in the tweets to be always categorized the same. Location identification uses an \ac{NER} model and
a gazetteer (i.e., a geographical dictionary), which are deterministic by nature if used on tweets
containing one toponym that refers to one geographical location. Given tweets containing more than
one toponym that refers to more than one location, the Swedish location containing the smallest
parameter is used. All the plots in the visual interface except for the scatter plot don't depend on
any randomness, making them reliable; the scatter plot will differ very slightly since it depends on
a dimensionality reduction method that returns the same overall structure intact with different
representations.

Validation is done by applying the pipeline to several use cases on different flood events and
checking the visual interface afterwards. If the methods are working as intended, the tweets table
should contain tweets discussing the concerned flood event, the map should have a spatial
distribution concentrated on the location of the event, the histogram representing the tweets'
creation date should spike at the time when the event started, dissipating afterwards, and the plots
presenting the text analysis techniques results should highlight some insights about the event. If
these indications are present for all the use cases, then the pipeline can be applied in other flood
events, given that they have sufficient coverage from Twitter users. If a flood event has too few to
no mentions on Twitter, this pipeline will not be able to identify it. The evaluation metrics in
Section~\ref{sec:text_classification_section} is another method to validate the classifier's
performance. This pipeline has the potential to generalize to other events besides floods in other
countries besides Sweden by doing minor tweaks; for the lack of time, the experiments done do not
validate these scenarios. As for the visual interface, exposing it to a variety of users, such as
college students and experts in \ac{SMHI} and getting feedback from them can assist in improving the
interface.

Using social media for research can be debated if it is ethical since the data sources are
individuals who most likely don't provide consent and are unaware that their content is used for
research \cite{zimmerItEthicalHarvest2010}. These social media users announce their ideas publicly,
yet they might not intend to contribute to something else. However, these users should be aware that
they are giving consent by agreeing to Twitter's terms of service which addresses the usage of the
content they create. \citeauth{kamockiLegalIssuesRelated2022} address the legal issues related to
intellectual property and personal data protection for reusing and sharing tweets. Depending on the
researcher's opinion on this matter, they might need to request the consent of countless individuals
before conducting research, which is a near-impossible task. Collecting a user's tweets might be
seen as invasive and harmful since it can generate a persona for malicious activities. This project
doesn't focus on individuals but on the flood-relevant content they generate, which can not harm the
users. Altering the methods in a way to focus on information outside the context of flood events,
such as sensitive data about individuals, can harm society. Fake messages posted for malicious
reasons are another ethical concern, and they can be handled by adding a fake news detection step to
the pipeline. Some tweets contain exaggerated content, which can make authorities distribute the
resources inappropriately during a hazard; to this end, decision-makers should take the information
provided by social media with a grain of salt and incorporate other sources before making a
decision. This project should is not a standalone tool for analysing a disaster;  it only provides a
magnifying glass on social media regarding the relevant details of critical situations.
